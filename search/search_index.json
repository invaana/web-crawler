{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"InvanaBot A spider cum data aggregation framework that can transform Websites, Feeds, APIs into Datasets with Crawl, Transform and Index strategy; with configurations instead of code. The framework comes with standard extractors that comes handy for most of the usecases. And gives you a way to write your own extraction strategy. The framework comes with integrations to MongoDB and Elasticsearch by default, allowing you to save your data into your preferred database. Some reasons you might want to use InvanaBot: No or least coding experience need, depending on the depth of your use-case. You can crawl a single site or traverse between multiple sites with just configuration. Data Storages like MongoDB and Elasticsearch are integrated already for use, Many more to join Built on top of Scrapy, so the core engine is Battle tested. Architecture InvanaBot strategy relies on Crawl + Transform + Index Strategy, allowing user to traverse between multiple domains using just configuration.. Requirements Built with Python 3.6.x, So python 2.7.x is not supported . While the support might expand to more versions of Python 3x in near future. We are not planning on supporting 2.7 as it is reaching its end of life soon. Installation Install using pip... pip install invana-bot ...or clone the project from github. pip install git+https://github.com/invanalabs/invana-bot#egg=invana_bot Quickstart Can't wait to get started? The quickstart guide is the fastest way to setup a spider up and running. Support For priority support please sign up for a professional or premium sponsorship plan. Get in touch with us at sales@invanalabs.ai For updates on InvanaBot, you may also want to follow the #InvanaBot or @InvanaLabs on Twitter. License Copyright \u00a9 2016-present, Invana Technology Solutions Pvt Ltd. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Home"},{"location":"#invanabot","text":"A spider cum data aggregation framework that can transform Websites, Feeds, APIs into Datasets with Crawl, Transform and Index strategy; with configurations instead of code. The framework comes with standard extractors that comes handy for most of the usecases. And gives you a way to write your own extraction strategy. The framework comes with integrations to MongoDB and Elasticsearch by default, allowing you to save your data into your preferred database. Some reasons you might want to use InvanaBot: No or least coding experience need, depending on the depth of your use-case. You can crawl a single site or traverse between multiple sites with just configuration. Data Storages like MongoDB and Elasticsearch are integrated already for use, Many more to join Built on top of Scrapy, so the core engine is Battle tested.","title":"InvanaBot"},{"location":"#architecture","text":"InvanaBot strategy relies on Crawl + Transform + Index Strategy, allowing user to traverse between multiple domains using just configuration..","title":"Architecture"},{"location":"#requirements","text":"Built with Python 3.6.x, So python 2.7.x is not supported . While the support might expand to more versions of Python 3x in near future. We are not planning on supporting 2.7 as it is reaching its end of life soon.","title":"Requirements"},{"location":"#installation","text":"Install using pip... pip install invana-bot ...or clone the project from github. pip install git+https://github.com/invanalabs/invana-bot#egg=invana_bot","title":"Installation"},{"location":"#quickstart","text":"Can't wait to get started? The quickstart guide is the fastest way to setup a spider up and running.","title":"Quickstart"},{"location":"#support","text":"For priority support please sign up for a professional or premium sponsorship plan. Get in touch with us at sales@invanalabs.ai For updates on InvanaBot, you may also want to follow the #InvanaBot or @InvanaLabs on Twitter.","title":"Support"},{"location":"#license","text":"Copyright \u00a9 2016-present, Invana Technology Solutions Pvt Ltd. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"__index/","text":"InvanaBot Documentation InvanaBot operates on Crawl => Transform => Index workflow. About manifest.json: cti_id : unique identifier used to init_spider : json config that tells, from where the crawling should start. start_urls spiders : a list of json based configurations that tells how to traverse and parse extractors : list of json configurations that tells spider what data should be extracted from a web page. traversals : list of json configurations that defines the pagination or which page to goto. transformations : a list of python functions that can take results of current job as input and returns cleaned_results as output. data_storages : a list of that tells to what data storage, cleaned_results from different transformations should be saved to Example of a full features manifest.json { \"cti_id\": \"invanalab_xyz\", \"init_spider\": { \"start_urls\": [ \"https://blog.scrapinghub.com\" ], \"spider_id\": \"blog-list\" }, \"spiders\": [ { \"spider_id\": \"blog-list\", \"extractors\": [ { \"extractor_type\": \"CustomContentExtractor\", \"data_selectors\": [ { \"id\": \"blogs\", \"selector\": \".post-listing .post-item\", \"selector_attribute\": \"element\", \"multiple\": true, \"child_selectors\": [ { \"id\": \"url\", \"selector\": \".post-header h2 a\", \"selector_type\": \"css\", \"selector_attribute\": \"href\", \"multiple\": false }, { \"id\": \"title\", \"selector\": \".post-header h2 a\", \"selector_type\": \"css\", \"selector_attribute\": \"text\", \"multiple\": false }, { \"id\": \"content\", \"selector\": \".post-content\", \"selector_type\": \"css\", \"selector_attribute\": \"html\", \"multiple\": false } ] } ] } ], \"traversals\": [ { \"traversal_type\": \"pagination\", \"pagination\": { \"selector\": \".next-posts-link\", \"selector_type\": \"css\", \"max_pages\": 20 }, \"next_spider_id\": \"blog-list\" }, { \"traversal_type\": \"link_from_field\", \"link_from_field\": { \"extractor_type\": \"CustomContentExtractor\", \"extractor_id\": \"url\" }, \"next_spider_id\": \"blog-detail\" } ] } ], \"transformations\": [ ], \"data_storages\": [ { \"db_connection_uri\": \"mongodb://127.0.0.1/spiders_data_index\", \"db_collection_name\": \"invanalabs_xyz\" }, { \"db_connection_uri\": \"mongodb://127.0.0.1/spiders_data_index\", \"db_collection_name\": \"invanalabs_xyz\" } ], \"callbacks\": [ ], \"context\": { } } Traversals Types { \"traversal_type\": \"pagination\", \"pagination\": { \"selector\": \".next-posts-link\", \"selector_type\": \"css\", \"max_pages\": 20 }, \"next_spider_id\": \"blog-list\" } { \"traversal_type\": \"same_domain\", \"same_domain\": { \"max_pages\": 1000 }, \"next_spider_id\": \"blog-list\" } { \"traversal_type\": \"link_from_field\", \"link_from_field\": { \"extractor_type\": \"CustomContentExtractor\", \"extractor_id\": \"url\" }, \"next_spider_id\": \"blog-detail\" } Callback { \"callback_id\": \"default\", \"data_storage_id\": \"default\", \"url\": \"http://localhost/api/callback\", \"request_type\": \"POST\", \"payload\": { }, \"headers\": { \"X-TOKEN\": \"abc123456789\" } }","title":"InvanaBot Documentation"},{"location":"__index/#invanabot-documentation","text":"InvanaBot operates on Crawl => Transform => Index workflow.","title":"InvanaBot Documentation"},{"location":"__index/#about-manifestjson","text":"cti_id : unique identifier used to init_spider : json config that tells, from where the crawling should start. start_urls spiders : a list of json based configurations that tells how to traverse and parse extractors : list of json configurations that tells spider what data should be extracted from a web page. traversals : list of json configurations that defines the pagination or which page to goto. transformations : a list of python functions that can take results of current job as input and returns cleaned_results as output. data_storages : a list of that tells to what data storage, cleaned_results from different transformations should be saved to Example of a full features manifest.json { \"cti_id\": \"invanalab_xyz\", \"init_spider\": { \"start_urls\": [ \"https://blog.scrapinghub.com\" ], \"spider_id\": \"blog-list\" }, \"spiders\": [ { \"spider_id\": \"blog-list\", \"extractors\": [ { \"extractor_type\": \"CustomContentExtractor\", \"data_selectors\": [ { \"id\": \"blogs\", \"selector\": \".post-listing .post-item\", \"selector_attribute\": \"element\", \"multiple\": true, \"child_selectors\": [ { \"id\": \"url\", \"selector\": \".post-header h2 a\", \"selector_type\": \"css\", \"selector_attribute\": \"href\", \"multiple\": false }, { \"id\": \"title\", \"selector\": \".post-header h2 a\", \"selector_type\": \"css\", \"selector_attribute\": \"text\", \"multiple\": false }, { \"id\": \"content\", \"selector\": \".post-content\", \"selector_type\": \"css\", \"selector_attribute\": \"html\", \"multiple\": false } ] } ] } ], \"traversals\": [ { \"traversal_type\": \"pagination\", \"pagination\": { \"selector\": \".next-posts-link\", \"selector_type\": \"css\", \"max_pages\": 20 }, \"next_spider_id\": \"blog-list\" }, { \"traversal_type\": \"link_from_field\", \"link_from_field\": { \"extractor_type\": \"CustomContentExtractor\", \"extractor_id\": \"url\" }, \"next_spider_id\": \"blog-detail\" } ] } ], \"transformations\": [ ], \"data_storages\": [ { \"db_connection_uri\": \"mongodb://127.0.0.1/spiders_data_index\", \"db_collection_name\": \"invanalabs_xyz\" }, { \"db_connection_uri\": \"mongodb://127.0.0.1/spiders_data_index\", \"db_collection_name\": \"invanalabs_xyz\" } ], \"callbacks\": [ ], \"context\": { } }","title":"About manifest.json:"},{"location":"__index/#traversals-types","text":"{ \"traversal_type\": \"pagination\", \"pagination\": { \"selector\": \".next-posts-link\", \"selector_type\": \"css\", \"max_pages\": 20 }, \"next_spider_id\": \"blog-list\" } { \"traversal_type\": \"same_domain\", \"same_domain\": { \"max_pages\": 1000 }, \"next_spider_id\": \"blog-list\" } { \"traversal_type\": \"link_from_field\", \"link_from_field\": { \"extractor_type\": \"CustomContentExtractor\", \"extractor_id\": \"url\" }, \"next_spider_id\": \"blog-detail\" }","title":"Traversals Types"},{"location":"__index/#callback","text":"{ \"callback_id\": \"default\", \"data_storage_id\": \"default\", \"url\": \"http://localhost/api/callback\", \"request_type\": \"POST\", \"payload\": { }, \"headers\": { \"X-TOKEN\": \"abc123456789\" } }","title":"Callback"},{"location":"extractors/advanced-usage/","text":"Advanced Usage Here is the documentation on advanced level usage. Selector Explained Running Extractor manually from invana_bot.extractors import ParagraphsExtractor # response should of data type : scrapy.http.response.html.HtmlResponse response = <self.response of the Spider> extracted_data = ParagraphsExtractor(response=response, extractor=None, extractor_id=\"paragraphs\").run()","title":"Advanced Usage"},{"location":"extractors/advanced-usage/#advanced-usage","text":"Here is the documentation on advanced level usage.","title":"Advanced Usage"},{"location":"extractors/advanced-usage/#selector-explained","text":"","title":"Selector Explained"},{"location":"extractors/advanced-usage/#running-extractor-manually","text":"from invana_bot.extractors import ParagraphsExtractor # response should of data type : scrapy.http.response.html.HtmlResponse response = <self.response of the Spider> extracted_data = ParagraphsExtractor(response=response, extractor=None, extractor_id=\"paragraphs\").run()","title":"Running Extractor manually"},{"location":"extractors/custom-extractor/","text":"Custom Extractor Custom extractor allows user to build their own extractor data selectors. CustomContentExtractor For custom extractor, You need to define what data to extract and how to save it in the db. You can either give just one or multiple selector data or add child_selectors to any selector. NOTE: Only one level of child_selector is only supported. Usage spiders: - spider_id: blog_list extractors: - extractor_type: CustomContentExtractor extractor_id: blog_list_parser data_selectors: - selector_id: blogs selector: \".post-listing .post-item\" selector_attribute: element multiple: true child_selectors: - selector_id: url selector: \".post-header h2 a\" selector_type: css selector_attribute: href multiple: false - selector_id: title selector: \".post-header h2 a\" selector_type: css selector_attribute: text multiple: false - selector_id: content selector: \".post-content\" selector_type: css selector_attribute: html multiple: false","title":"Custom Extractor"},{"location":"extractors/custom-extractor/#custom-extractor","text":"Custom extractor allows user to build their own extractor data selectors.","title":"Custom Extractor"},{"location":"extractors/custom-extractor/#customcontentextractor","text":"For custom extractor, You need to define what data to extract and how to save it in the db. You can either give just one or multiple selector data or add child_selectors to any selector. NOTE: Only one level of child_selector is only supported.","title":"CustomContentExtractor"},{"location":"extractors/custom-extractor/#usage","text":"spiders: - spider_id: blog_list extractors: - extractor_type: CustomContentExtractor extractor_id: blog_list_parser data_selectors: - selector_id: blogs selector: \".post-listing .post-item\" selector_attribute: element multiple: true child_selectors: - selector_id: url selector: \".post-header h2 a\" selector_type: css selector_attribute: href multiple: false - selector_id: title selector: \".post-header h2 a\" selector_type: css selector_attribute: text multiple: false - selector_id: content selector: \".post-content\" selector_type: css selector_attribute: html multiple: false","title":"Usage"},{"location":"extractors/pythonic-extractor/","text":"Pythonic Extractor This extractor is unique from the rest of the extractors, because it gives more power to the directly to the user, giving full access to html content. The extractor is available at invana_bot.extractors.PythonBasedExtractor . User has to write a python function in ib_functions.py and the name of the function should be specified in the manifest.yml as shown in the example. # in manifest.yml - spider_id: default_spider allowed_domains: - \"github.com\" extractors: - extractor_id: page_detection extractor_type: PythonBasedExtractor extractor_fn: default_extractor_fn # ib_functions.py def default_extractor_fn(response=None): \"\"\" \"\"\" url = response.url data = {} if \"/contact\" in url: data[\"page_type\"] = \"contact\" elif \"/blog/\" in url: data[\"page_type\"] = \"blog\" elif \"/about\" in url: data[\"page_type\"] = \"about\" elif \"/service\" in url: data[\"page_type\"] = \"service\" elif \"/product\" in url: data[\"page_type\"] = \"product\" elif url.strip(\"/\").count(\"/\") == 2: # this can be improved. data[\"page_type\"] = \"homepage\" else: data[\"page_type\"] = \"others\" return data","title":"Pythonic Extractor"},{"location":"extractors/pythonic-extractor/#pythonic-extractor","text":"This extractor is unique from the rest of the extractors, because it gives more power to the directly to the user, giving full access to html content. The extractor is available at invana_bot.extractors.PythonBasedExtractor . User has to write a python function in ib_functions.py and the name of the function should be specified in the manifest.yml as shown in the example. # in manifest.yml - spider_id: default_spider allowed_domains: - \"github.com\" extractors: - extractor_id: page_detection extractor_type: PythonBasedExtractor extractor_fn: default_extractor_fn # ib_functions.py def default_extractor_fn(response=None): \"\"\" \"\"\" url = response.url data = {} if \"/contact\" in url: data[\"page_type\"] = \"contact\" elif \"/blog/\" in url: data[\"page_type\"] = \"blog\" elif \"/about\" in url: data[\"page_type\"] = \"about\" elif \"/service\" in url: data[\"page_type\"] = \"service\" elif \"/product\" in url: data[\"page_type\"] = \"product\" elif url.strip(\"/\").count(\"/\") == 2: # this can be improved. data[\"page_type\"] = \"homepage\" else: data[\"page_type\"] = \"others\" return data","title":"Pythonic Extractor"},{"location":"extractors/standard-extractors/","text":"Standard Extractor InvanaBot allows users to extract information while it crawls through the webpages. You can specify multiple extractors to a spider, allowing you to organise the information you need into grouped/subdocument data. All the extractors are available at invana_bot.extractors ParagraphsExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: ParagraphsExtractor extractor_id: paragraphs_data Here is the data extracted. This will return all the paragraphs as list. { \"paragraphs_data\" : [ \"Here is the first paragraph\", \"Here is the second paragraph\" ] } TableContentExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: TableContentExtractor extractor_id: tables_data Here is the data extracted. This will return all the tables as list. { \"tables_data\":[ [ // table 1 { \"Code\":\"IN\", \"Country\":\"India\", \"Last Checked\":\"1 second ago\" } ], [ // table 2 { \"Code\": \"DEL\", \"State\": \"New Delhi\" } ] ] } MetaTagExtractor Here is the configuration. This will gather data of og, twitter and fb meta tags, pretty much any meta element. spiders: - spider_id: default_spider extractors: - extractor_type: MetaTagExtractor extractor_id: meta_tag_data Here is the data extracted. This will return all the <meta> tags data as dictionary. { \"meta_tag_data\":{ \"meta__viewport\":\"width=device-width,initial-scale=1,maximum-scale=1\", \"meta__referrer\":\"origin\", \"meta__description\":\"Here is the description of the site \", \"title\":\"Example site | Homepage\" } } IconsExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: IconsExtractor extractor_id: icon_data Here is the data extracted. This will return all the <meta> tags data as dictionary. { \"icon_data\": { \"32x32\": \"https://example.com/2018/10/fav-icon.png?w=32\", \"192x192\": \"https://example.com/2018/10/fav-icon.png?w=120\" } } JSONLDExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: JSONLDExtractor extractor_id: json_ld_data Here is the data extracted. The returned data would be in list, as there can be multiple json+ld descriptions in a single page. { \"json_ld_data\": [ { \"@context\": \"http://schema.org\", \"@type\": \"NewsArticle\", \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://example.com/article/technology/science/space-age/\" }, \"url\": \"https://example.com/article/technology/science/space-age/\", \"articleBody\": \"Lorem ipusum, here is the article body.\", \"articleSection\": \"technology\", \"keywords\": \"Chandrayaan-2, Chandrayaan-2 launch, Chandrayaan-2 launch monday, Chandrayaan-2 launch timing, Chandrayaan-2 isro launch date time, Chandrayaan-2 launch july 15, isro moon, isro Chandrayaan-2, Chandrayaan-2 moon\", \"headline\": \"Chandrayaan-2 to launch India into new space age\", \"description\": \"The Chandrayaan-2, a moon-lander and rover mission, is designed to go where no spacecraft has gone before.\", \"datePublished\": \"2019-07-14T09:29:56+05:30\", \"dateModified\": \"2019-07-14T09:29:56+05:30\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"MoonNews Publications\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://s2.wp.com/wp-content/themes/vip/example.com-v2/dist/images/ienewlogo3_new.png\", \"width\": \"600\", \"height\": \"60\" } }, \"author\": { \"@type\": \"Person\", \"name\": \"John Doe\", \"sameAs\": \"https://example.com/profile/author/john-doe/\" }, \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://images.example.com/2019/07/chandrayaan-2_759-1.jpg\", \"width\": \"759\", \"height\": \"422\" } }, { \"@context\": \"http://schema.org\", \"@type\": \"Person\", \"name\": \"John Doe\", \"url\": \"https://example.com/profile/author/john-doe/\", \"worksFor\": { \"@type\": \"Organization\", \"name\": \"MoonNews Publications\", \"url\": \"https://example.com/\" } }, { \"@context\": \"http://schema.org\", \"@type\": \"WebSite\", \"url\": \"https://example.com/\", \"potentialAction\": { \"@type\": \"SearchAction\", \"target\": \"https://example.com/?s={search_term_string}\", \"query-input\": \"required name=search_term_string\" } } ] } PlainHTMLContentExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: PlainHTMLContentExtractor extractor_id: html_content Here is the data extracted. { \"html_content\": \"<html></body>Hello World!</body></html>\" } PageOverviewExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview Here is the data extracted. { \"overview\":{ \"title\":\"Welcome to the site.\", \"description\":\"Our space mission is designed to go where no spacecraft has gone before.\", \"image\":\"https://example.com/2019/07/image.jpg?w=759\", \"url\":\"https://example.com/article/technology/science/space-age/\", \"page_type\":\"article\", \"keywords\":\"space launch, chandrayaan-2 launch\", \"domain\":\"example.com\", \"first_paragraph\":\"First Paragraph comes here.\", \"shortlink_url\":\"https://example.com/?page=ok-me\", \"canonical_url\":\"https://example.com/article/technology/science/space-age/\" } } FeedUrlExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: FeedUrlExtractor extractor_id: feeds_data Here is the data extracted. { \"feeds_data\": {\"rss__xml\": \"https://blog.scrapinghub.com/rss.xml\", \"rss__atom\": null} } ImagesExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: ImagesExtractor extractor_id: images Here is the data extracted. { \"images\": [ \"https://example.com/image-1.png\", \"https://example.com/image-2.png\", \"https://example.com/image-3.png\" ] } AllLinksExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: AllLinksExtractor extractor_id: all_links Here is the data extracted. This will contains all links { \"all_links\": [ \"https://example.com/page-1\", \"https://example.com/page-2\", \"https://example.com/page-3\", \"https://facebook.com/page-3\", \"https://twitter.com/page-3\" ] } AllLinksAnalyticsExtractor Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: AllLinksAnalyticsExtractor extractor_id: all_links_analysed Here is the data extracted. This will contains all links in the page, seperating them into domain specific links. { \"links\":[ { \"domain\":\"blog.scrapinghub.com\", \"links\":[ \"https://blog.scrapinghub.com\", \"https://blog.scrapinghub.com/web-data-analysis-exposing-nfl-player-salaries-with-python\", \"https://blog.scrapinghub.com/author/attila-t\u00f3th\", \"https://blog.scrapinghub.com/web-data-analysis-exposing-nfl-player-salaries-with-python#comments-listing\", \"https://blog.scrapinghub.com/spidermon-scrapy-spider-monitoring\", \"...\", \"https://blog.scrapinghub.com/alternative-financial-data-quality\" ], \"links_count\":48 }, { \"domain\":\"overthecap.com\", \"links\":[ \"https://overthecap.com/\", \"https://overthecap.com/position/quarterback/\" ], \"links_count\":2 }, { \"domain\":\"github.com\", \"links\":[ \"https://github.com/zseta/NFL-Contracts\", \"https://github.com/zseta/NFL-Contracts\", \"https://github.com/zseta/NFL-Contracts\" ], \"links_count\":3 }, { \"domain\":\"scrapingauthority.com\", \"links\":[ \"https://scrapingauthority.com/resources/\" ], \"links_count\":1 }, { \"domain\":\"twitter.com\", \"links\":[ \"https://twitter.com/share\", \"https://twitter.com/ScrapingHub\" ], \"links_count\":2 } ] }","title":"Standard Extractors"},{"location":"extractors/standard-extractors/#standard-extractor","text":"InvanaBot allows users to extract information while it crawls through the webpages. You can specify multiple extractors to a spider, allowing you to organise the information you need into grouped/subdocument data. All the extractors are available at invana_bot.extractors","title":"Standard Extractor"},{"location":"extractors/standard-extractors/#paragraphsextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: ParagraphsExtractor extractor_id: paragraphs_data Here is the data extracted. This will return all the paragraphs as list. { \"paragraphs_data\" : [ \"Here is the first paragraph\", \"Here is the second paragraph\" ] }","title":"ParagraphsExtractor"},{"location":"extractors/standard-extractors/#tablecontentextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: TableContentExtractor extractor_id: tables_data Here is the data extracted. This will return all the tables as list. { \"tables_data\":[ [ // table 1 { \"Code\":\"IN\", \"Country\":\"India\", \"Last Checked\":\"1 second ago\" } ], [ // table 2 { \"Code\": \"DEL\", \"State\": \"New Delhi\" } ] ] }","title":"TableContentExtractor"},{"location":"extractors/standard-extractors/#metatagextractor","text":"Here is the configuration. This will gather data of og, twitter and fb meta tags, pretty much any meta element. spiders: - spider_id: default_spider extractors: - extractor_type: MetaTagExtractor extractor_id: meta_tag_data Here is the data extracted. This will return all the <meta> tags data as dictionary. { \"meta_tag_data\":{ \"meta__viewport\":\"width=device-width,initial-scale=1,maximum-scale=1\", \"meta__referrer\":\"origin\", \"meta__description\":\"Here is the description of the site \", \"title\":\"Example site | Homepage\" } }","title":"MetaTagExtractor"},{"location":"extractors/standard-extractors/#iconsextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: IconsExtractor extractor_id: icon_data Here is the data extracted. This will return all the <meta> tags data as dictionary. { \"icon_data\": { \"32x32\": \"https://example.com/2018/10/fav-icon.png?w=32\", \"192x192\": \"https://example.com/2018/10/fav-icon.png?w=120\" } }","title":"IconsExtractor"},{"location":"extractors/standard-extractors/#jsonldextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: JSONLDExtractor extractor_id: json_ld_data Here is the data extracted. The returned data would be in list, as there can be multiple json+ld descriptions in a single page. { \"json_ld_data\": [ { \"@context\": \"http://schema.org\", \"@type\": \"NewsArticle\", \"mainEntityOfPage\": { \"@type\": \"WebPage\", \"@id\": \"https://example.com/article/technology/science/space-age/\" }, \"url\": \"https://example.com/article/technology/science/space-age/\", \"articleBody\": \"Lorem ipusum, here is the article body.\", \"articleSection\": \"technology\", \"keywords\": \"Chandrayaan-2, Chandrayaan-2 launch, Chandrayaan-2 launch monday, Chandrayaan-2 launch timing, Chandrayaan-2 isro launch date time, Chandrayaan-2 launch july 15, isro moon, isro Chandrayaan-2, Chandrayaan-2 moon\", \"headline\": \"Chandrayaan-2 to launch India into new space age\", \"description\": \"The Chandrayaan-2, a moon-lander and rover mission, is designed to go where no spacecraft has gone before.\", \"datePublished\": \"2019-07-14T09:29:56+05:30\", \"dateModified\": \"2019-07-14T09:29:56+05:30\", \"publisher\": { \"@type\": \"Organization\", \"name\": \"MoonNews Publications\", \"logo\": { \"@type\": \"ImageObject\", \"url\": \"https://s2.wp.com/wp-content/themes/vip/example.com-v2/dist/images/ienewlogo3_new.png\", \"width\": \"600\", \"height\": \"60\" } }, \"author\": { \"@type\": \"Person\", \"name\": \"John Doe\", \"sameAs\": \"https://example.com/profile/author/john-doe/\" }, \"image\": { \"@type\": \"ImageObject\", \"url\": \"https://images.example.com/2019/07/chandrayaan-2_759-1.jpg\", \"width\": \"759\", \"height\": \"422\" } }, { \"@context\": \"http://schema.org\", \"@type\": \"Person\", \"name\": \"John Doe\", \"url\": \"https://example.com/profile/author/john-doe/\", \"worksFor\": { \"@type\": \"Organization\", \"name\": \"MoonNews Publications\", \"url\": \"https://example.com/\" } }, { \"@context\": \"http://schema.org\", \"@type\": \"WebSite\", \"url\": \"https://example.com/\", \"potentialAction\": { \"@type\": \"SearchAction\", \"target\": \"https://example.com/?s={search_term_string}\", \"query-input\": \"required name=search_term_string\" } } ] }","title":"JSONLDExtractor"},{"location":"extractors/standard-extractors/#plainhtmlcontentextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: PlainHTMLContentExtractor extractor_id: html_content Here is the data extracted. { \"html_content\": \"<html></body>Hello World!</body></html>\" }","title":"PlainHTMLContentExtractor"},{"location":"extractors/standard-extractors/#pageoverviewextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview Here is the data extracted. { \"overview\":{ \"title\":\"Welcome to the site.\", \"description\":\"Our space mission is designed to go where no spacecraft has gone before.\", \"image\":\"https://example.com/2019/07/image.jpg?w=759\", \"url\":\"https://example.com/article/technology/science/space-age/\", \"page_type\":\"article\", \"keywords\":\"space launch, chandrayaan-2 launch\", \"domain\":\"example.com\", \"first_paragraph\":\"First Paragraph comes here.\", \"shortlink_url\":\"https://example.com/?page=ok-me\", \"canonical_url\":\"https://example.com/article/technology/science/space-age/\" } }","title":"PageOverviewExtractor"},{"location":"extractors/standard-extractors/#feedurlextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: FeedUrlExtractor extractor_id: feeds_data Here is the data extracted. { \"feeds_data\": {\"rss__xml\": \"https://blog.scrapinghub.com/rss.xml\", \"rss__atom\": null} }","title":"FeedUrlExtractor"},{"location":"extractors/standard-extractors/#imagesextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: ImagesExtractor extractor_id: images Here is the data extracted. { \"images\": [ \"https://example.com/image-1.png\", \"https://example.com/image-2.png\", \"https://example.com/image-3.png\" ] }","title":"ImagesExtractor"},{"location":"extractors/standard-extractors/#alllinksextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: AllLinksExtractor extractor_id: all_links Here is the data extracted. This will contains all links { \"all_links\": [ \"https://example.com/page-1\", \"https://example.com/page-2\", \"https://example.com/page-3\", \"https://facebook.com/page-3\", \"https://twitter.com/page-3\" ] }","title":"AllLinksExtractor"},{"location":"extractors/standard-extractors/#alllinksanalyticsextractor","text":"Here is the configuration. spiders: - spider_id: default_spider extractors: - extractor_type: AllLinksAnalyticsExtractor extractor_id: all_links_analysed Here is the data extracted. This will contains all links in the page, seperating them into domain specific links. { \"links\":[ { \"domain\":\"blog.scrapinghub.com\", \"links\":[ \"https://blog.scrapinghub.com\", \"https://blog.scrapinghub.com/web-data-analysis-exposing-nfl-player-salaries-with-python\", \"https://blog.scrapinghub.com/author/attila-t\u00f3th\", \"https://blog.scrapinghub.com/web-data-analysis-exposing-nfl-player-salaries-with-python#comments-listing\", \"https://blog.scrapinghub.com/spidermon-scrapy-spider-monitoring\", \"...\", \"https://blog.scrapinghub.com/alternative-financial-data-quality\" ], \"links_count\":48 }, { \"domain\":\"overthecap.com\", \"links\":[ \"https://overthecap.com/\", \"https://overthecap.com/position/quarterback/\" ], \"links_count\":2 }, { \"domain\":\"github.com\", \"links\":[ \"https://github.com/zseta/NFL-Contracts\", \"https://github.com/zseta/NFL-Contracts\", \"https://github.com/zseta/NFL-Contracts\" ], \"links_count\":3 }, { \"domain\":\"scrapingauthority.com\", \"links\":[ \"https://scrapingauthority.com/resources/\" ], \"links_count\":1 }, { \"domain\":\"twitter.com\", \"links\":[ \"https://twitter.com/share\", \"https://twitter.com/ScrapingHub\" ], \"links_count\":2 } ] }","title":"AllLinksAnalyticsExtractor"},{"location":"spiders/api-spider/","text":"API Spider Create a manifest.yml and run the spider. cti_id: localhost-auditlogs init_spider: spider_id: default start_urls: - http://localhost:8000/api/index/spider/data?page=1&crawled_id=bing_search&extractor_id=bing-search-result&job_id=5d25d414b7ba3e4972172913&token=b1d0801131a542d98a492916da362612 spiders: - spider_id: default allowed_domains: - localhost:8000 extra_url_param: crawled_id=bing_search&extractor_id=bing-search-result&job_id=5d25d414b7ba3e4972172913&token=b1d0801131a542d98a492916da362612 pagination_param: page result_key: result settings: allowed_domains: - \"localhost:8000\" download_delay: 0 context: author: https://github.com/rrmerugu description: Crawler that scrapes audit logs Running the Spider invana-bot --type=api","title":"3. API Spider"},{"location":"spiders/api-spider/#api-spider","text":"Create a manifest.yml and run the spider. cti_id: localhost-auditlogs init_spider: spider_id: default start_urls: - http://localhost:8000/api/index/spider/data?page=1&crawled_id=bing_search&extractor_id=bing-search-result&job_id=5d25d414b7ba3e4972172913&token=b1d0801131a542d98a492916da362612 spiders: - spider_id: default allowed_domains: - localhost:8000 extra_url_param: crawled_id=bing_search&extractor_id=bing-search-result&job_id=5d25d414b7ba3e4972172913&token=b1d0801131a542d98a492916da362612 pagination_param: page result_key: result settings: allowed_domains: - \"localhost:8000\" download_delay: 0 context: author: https://github.com/rrmerugu description: Crawler that scrapes audit logs","title":"API Spider"},{"location":"spiders/api-spider/#running-the-spider","text":"invana-bot --type=api","title":"Running the Spider"},{"location":"spiders/feeds-spider/","text":"Feeds Spider Create a manifest.yml and run the spider. cti_id: news.ycombinator.com init_spider: spider_id: default start_urls: - https://news.ycombinator.com/rss spiders: - spider_id: default allowed_domains: - news.ycombinator.com iterator: xml itertag: item extractors: - extractor_id: stories extractor_type: CustomContentExtractor data_selectors: - selector_id: title selector: title selector_type: xpath selector_attribute: text() data_type: StringField - selector_id: link selector: link selector_type: xpath selector_attribute: text() data_type: StringField - selector_id: published_date selector: pubDate selector_type: xpath selector_attribute: text() data_type: StringField settings: allowed_domains: - \"news.ycombinator.com\" download_delay: 0 context: author: https://github.com/rrmerugu description: Crawler that scrapes ycombinator news Running the Spider invana-bot --type=rss","title":"2. Feeds Spider"},{"location":"spiders/feeds-spider/#feeds-spider","text":"Create a manifest.yml and run the spider. cti_id: news.ycombinator.com init_spider: spider_id: default start_urls: - https://news.ycombinator.com/rss spiders: - spider_id: default allowed_domains: - news.ycombinator.com iterator: xml itertag: item extractors: - extractor_id: stories extractor_type: CustomContentExtractor data_selectors: - selector_id: title selector: title selector_type: xpath selector_attribute: text() data_type: StringField - selector_id: link selector: link selector_type: xpath selector_attribute: text() data_type: StringField - selector_id: published_date selector: pubDate selector_type: xpath selector_attribute: text() data_type: StringField settings: allowed_domains: - \"news.ycombinator.com\" download_delay: 0 context: author: https://github.com/rrmerugu description: Crawler that scrapes ycombinator news","title":"Feeds Spider"},{"location":"spiders/feeds-spider/#running-the-spider","text":"invana-bot --type=rss","title":"Running the Spider"},{"location":"spiders/manifest/","text":"Manifest InvanaBot crawls, transforms and data_storages the data from multiple web sources. Spiders InvanaBot supports 3 types of spiders - 1. web 2. Feeds 3. API. There is no difference in the manifest for web, feeds or API. The difference exists only in the way you execute invana-bot command. - spider_id: default_spider allowed_domains: - blog.scrapinghub.com Extractors - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview Traversals This will help the spiders traverse from one domain to the other or with-in the domain. - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview traversals: - traversal_id: bing_pagination selector_value: a.sb_pagN selector_type: css max_pages: 5 next_spider_id: bing_search Transformations This is the transformation that can be applied on all the data crawled from a single run. This will be executed after the whole job is done. transformation_id - name of the transformation transformation_fn - string of python code with a function with the name of transformation_id. Here is the transformation config: - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview transformations: - transformation_id: default_transformation transformation_fn : \"\"\" #!/usr/bin/env python def default_transformation(data): # TODO modify the data according to your needs. return data \"\"\" Data Storages This is where the data extracted during the spiders will be saved. Currently InvanaBot supports MongoDB and Elasticsearch . Example usage - spider_id: default_spider allowed_domains: - blog.scrapinghub.com extractors: - extractor_type: PageOverviewExtractor extractor_id: overview transformations: - transformation_id: default_transformation transformation_fn : \"\"\" #!/usr/bin/env python def default_transformation(data): # TODO modify the data according to your needs. return data \"\"\" data_storages: - data_storage_id: default_index transformation_id: default_transformation connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: default_spider unique_key: url Controllers DOWNLOAD_TIMEOUT Default is 180 This is the max time a downloader would wait for the response beforing calling timeout. CONCURRENT_ITEMS Default is","title":"manifest.yml"},{"location":"spiders/manifest/#manifest","text":"InvanaBot crawls, transforms and data_storages the data from multiple web sources.","title":"Manifest"},{"location":"spiders/manifest/#spiders","text":"InvanaBot supports 3 types of spiders - 1. web 2. Feeds 3. API. There is no difference in the manifest for web, feeds or API. The difference exists only in the way you execute invana-bot command. - spider_id: default_spider allowed_domains: - blog.scrapinghub.com","title":"Spiders"},{"location":"spiders/manifest/#extractors","text":"- spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview","title":"Extractors"},{"location":"spiders/manifest/#traversals","text":"This will help the spiders traverse from one domain to the other or with-in the domain. - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview traversals: - traversal_id: bing_pagination selector_value: a.sb_pagN selector_type: css max_pages: 5 next_spider_id: bing_search","title":"Traversals"},{"location":"spiders/manifest/#transformations","text":"This is the transformation that can be applied on all the data crawled from a single run. This will be executed after the whole job is done. transformation_id - name of the transformation transformation_fn - string of python code with a function with the name of transformation_id. Here is the transformation config: - spider_id: default_spider extractors: - extractor_type: PageOverviewExtractor extractor_id: overview transformations: - transformation_id: default_transformation transformation_fn : \"\"\" #!/usr/bin/env python def default_transformation(data): # TODO modify the data according to your needs. return data \"\"\"","title":"Transformations"},{"location":"spiders/manifest/#data-storages","text":"This is where the data extracted during the spiders will be saved. Currently InvanaBot supports MongoDB and Elasticsearch .","title":"Data Storages"},{"location":"spiders/manifest/#example-usage","text":"- spider_id: default_spider allowed_domains: - blog.scrapinghub.com extractors: - extractor_type: PageOverviewExtractor extractor_id: overview transformations: - transformation_id: default_transformation transformation_fn : \"\"\" #!/usr/bin/env python def default_transformation(data): # TODO modify the data according to your needs. return data \"\"\" data_storages: - data_storage_id: default_index transformation_id: default_transformation connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: default_spider unique_key: url","title":"Example usage"},{"location":"spiders/manifest/#controllers","text":"","title":"Controllers"},{"location":"spiders/manifest/#download_timeout","text":"Default is 180 This is the max time a downloader would wait for the response beforing calling timeout.","title":"DOWNLOAD_TIMEOUT"},{"location":"spiders/manifest/#concurrent_items","text":"Default is","title":"CONCURRENT_ITEMS"},{"location":"spiders/web-spider/","text":"Web Spider Create a manifest.yml and run the spider. Creating the manifest.yml init_spider: start_urls: - \"https://www.bing.com/search?q=amazon.com:moto g5\" spider_id: bing_search spiders: - spider_id: bing_search allowed_domains: - bing.com extractors: - extractor_type: MetaTagExtractor extractor_id: bing_search_result traversals: - traversal_id: amazon_spider_traversal selector_type: css selector_value: \".b_algo h2 a\" next_spider_id: amazon_spider max_pages: 1 - spider_id: amazon_spider allowed_domains: - amazon.in - amazon.com extractors: - extractor_type: CustomContentExtractor extractor_id: seo_data2 data_selectors: - selector_id: title selector: title selector_type: css selector_attribute: text multiple: false - selector_id: description selector: \"//meta[@name='description']\" selector_type: xpath selector_attribute: \"@content\" multiple: false - selector_id: og_description selector: \"//meta[@name='og:description']\" selector_type: xpath selector_attribute: \"@content\" multiple: false - extractor_type: MetaTagExtractor extractor_id: seo_data settings: allowed_domains: - bing.com - amazon.in - amazon.com download_delay: 0 context: cti_id: tcl-agriculture cti_id: tcl-agriculture Running the Spider invana-bot --type=web","title":"1. Web Spider"},{"location":"spiders/web-spider/#web-spider","text":"Create a manifest.yml and run the spider.","title":"Web Spider"},{"location":"spiders/web-spider/#creating-the-manifestyml","text":"init_spider: start_urls: - \"https://www.bing.com/search?q=amazon.com:moto g5\" spider_id: bing_search spiders: - spider_id: bing_search allowed_domains: - bing.com extractors: - extractor_type: MetaTagExtractor extractor_id: bing_search_result traversals: - traversal_id: amazon_spider_traversal selector_type: css selector_value: \".b_algo h2 a\" next_spider_id: amazon_spider max_pages: 1 - spider_id: amazon_spider allowed_domains: - amazon.in - amazon.com extractors: - extractor_type: CustomContentExtractor extractor_id: seo_data2 data_selectors: - selector_id: title selector: title selector_type: css selector_attribute: text multiple: false - selector_id: description selector: \"//meta[@name='description']\" selector_type: xpath selector_attribute: \"@content\" multiple: false - selector_id: og_description selector: \"//meta[@name='og:description']\" selector_type: xpath selector_attribute: \"@content\" multiple: false - extractor_type: MetaTagExtractor extractor_id: seo_data settings: allowed_domains: - bing.com - amazon.in - amazon.com download_delay: 0 context: cti_id: tcl-agriculture cti_id: tcl-agriculture","title":"Creating the manifest.yml"},{"location":"spiders/web-spider/#running-the-spider","text":"invana-bot --type=web","title":"Running the Spider"},{"location":"topics/terminology/","text":"Terminology spiders : single unit of CTI flow. A spider has extractors, traversals. traversal : definition of how one spider will navigate to another one. extractor : Definition of what data to extract from the web page and how it should be saved in to the database. Extractor will have a definition of multiple selectors that should be extracted. selector : Definition of a unit data that will be extracted by extractor. transformation : Instructions that tells how to convert crawled data into a final data. This is the step where user can clean or convert the data into the desired formats. callbacks : Callbacks that should be pinged once the job is done, so that other jobs can be triggered by integrations. context : This data will added to each entry that is saved by the spider. This data is something you might want to carry forward to all the crawling jobs and saved entries. Example: context = {\"author\" : \"rrmerugu\", \"project_id\": \"alien-data\"} will save this in all the crawl extracted data.","title":"Terminology"},{"location":"topics/terminology/#terminology","text":"spiders : single unit of CTI flow. A spider has extractors, traversals. traversal : definition of how one spider will navigate to another one. extractor : Definition of what data to extract from the web page and how it should be saved in to the database. Extractor will have a definition of multiple selectors that should be extracted. selector : Definition of a unit data that will be extracted by extractor. transformation : Instructions that tells how to convert crawled data into a final data. This is the step where user can clean or convert the data into the desired formats. callbacks : Callbacks that should be pinged once the job is done, so that other jobs can be triggered by integrations. context : This data will added to each entry that is saved by the spider. This data is something you might want to carry forward to all the crawling jobs and saved entries. Example: context = {\"author\" : \"rrmerugu\", \"project_id\": \"alien-data\"} will save this in all the crawl extracted data.","title":"Terminology"},{"location":"topics/write-own-data-storage/","text":"Write own Data Storage","title":"Write own data storage"},{"location":"topics/write-own-data-storage/#write-own-data-storage","text":"","title":"Write own Data Storage"},{"location":"transformations/doc-transformation/","text":"Document Transformation","title":"2. Doc Transformation"},{"location":"transformations/doc-transformation/#document-transformation","text":"","title":"Document Transformation"},{"location":"transformations/field-transformation/","text":"Field Transformation","title":"1. Field Transformation"},{"location":"transformations/field-transformation/#field-transformation","text":"","title":"Field Transformation"},{"location":"transformations/job-transformation/","text":"Document Transformation","title":"3. Job Transformation"},{"location":"transformations/job-transformation/#document-transformation","text":"","title":"Document Transformation"},{"location":"transformations/quickstart/","text":"Quick start You can apply multiple transformations on the crawled data. Transformations are applied once all the spiders in the CTI flow are done. Here is how you define a transformation. You need to add the transformation config in yaml . Each trasnformation would contain two things: transformation_id : name of the transformation(sluggified version is the only right way now ) transformation_fn : name of the python function in the cti_transformations.py (for cti flow) or spider_transformations.py (for single spider) # cti_manifest.yml or spider_manifest.yml transformations: - transformation_id: default transformation_fn: transformation_fn - data_storage_id: primary_db transformation_id: default connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: blog_list unique_key: url # cti_transformations.py or spider_transformations.py def transformation_fn(results): \"\"\" results will contain all the documents that are stored in the database during a given cti/spider job. if you want to identify the data of a parser data with in the results. You can use the example below. \"\"\" results_cleaned = [] for result in results: blog_list_parser_data = result.get(\"blog_list_parser\",{}) if blog_list_parser_data: for blog in blog_list_parser_data.get(\"blogs\",[]): results_cleaned.append(blog) return results_cleaned","title":"QuickStart"},{"location":"transformations/quickstart/#quick-start","text":"You can apply multiple transformations on the crawled data. Transformations are applied once all the spiders in the CTI flow are done. Here is how you define a transformation. You need to add the transformation config in yaml . Each trasnformation would contain two things: transformation_id : name of the transformation(sluggified version is the only right way now ) transformation_fn : name of the python function in the cti_transformations.py (for cti flow) or spider_transformations.py (for single spider) # cti_manifest.yml or spider_manifest.yml transformations: - transformation_id: default transformation_fn: transformation_fn - data_storage_id: primary_db transformation_id: default connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: blog_list unique_key: url # cti_transformations.py or spider_transformations.py def transformation_fn(results): \"\"\" results will contain all the documents that are stored in the database during a given cti/spider job. if you want to identify the data of a parser data with in the results. You can use the example below. \"\"\" results_cleaned = [] for result in results: blog_list_parser_data = result.get(\"blog_list_parser\",{}) if blog_list_parser_data: for blog in blog_list_parser_data.get(\"blogs\",[]): results_cleaned.append(blog) return results_cleaned","title":"Quick start"},{"location":"tutorials/api-crawler/","text":"API Crawler","title":"API Crawler"},{"location":"tutorials/api-crawler/#api-crawler","text":"","title":"API Crawler"},{"location":"tutorials/feeds-crawler/","text":"Feeds Crawler","title":"Feeds Crawler"},{"location":"tutorials/feeds-crawler/#feeds-crawler","text":"","title":"Feeds Crawler"},{"location":"tutorials/multi-domain-crawler/","text":"Multi Domain Crawler Crawl, Traverse and Index # spider_manifest.yml spider_id: blog_list whitelisted_domains: - blog.scrapinghub.com start_urls: - https://blog.scrapinghub.com extractors: - extractor_type: MetaTagExtractor extractor_id: meta_tags - extractor_type: ParagraphsExtractor extractor_id: paragraphs - extractor_type: CustomContentExtractor extractor_id: blog_list_parser data_selectors: - selector_id: blogs selector: \".post-listing .post-item\" selector_attribute: element multiple: true child_selectors: - selector_id: url selector: \".post-header h2 a\" selector_type: css selector_attribute: href multiple: false - selector_id: title selector: \".post-header h2 a\" selector_type: css selector_attribute: text multiple: false - selector_id: content selector: \".post-content\" selector_type: css selector_attribute: html multiple: false traversals: - traversal_type: pagination pagination: selector: \".next-posts-link\" selector_type: css max_pages: 2 next_spider_id: blog_list transformations: - transformation_id: default transformation_fn: transformation_fn callbacks: - callback_id: default data_storage_id: default url: http://localhost/api/callback request_type: POST payload: {} headers: X-TOKEN: abc123456789 data_storages: - data_storage_id: default transformation_id: default connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: blog_list unique_key: url settings: allowed_domains: - blog.scrapinghub.com download_delay: 5 context: author: https://github.com/rrmerugu description: Crawler that scrapes scrapinghub blogs","title":"Multi Domain Crawler"},{"location":"tutorials/multi-domain-crawler/#multi-domain-crawler","text":"","title":"Multi Domain Crawler"},{"location":"tutorials/multi-domain-crawler/#crawl-traverse-and-index","text":"# spider_manifest.yml spider_id: blog_list whitelisted_domains: - blog.scrapinghub.com start_urls: - https://blog.scrapinghub.com extractors: - extractor_type: MetaTagExtractor extractor_id: meta_tags - extractor_type: ParagraphsExtractor extractor_id: paragraphs - extractor_type: CustomContentExtractor extractor_id: blog_list_parser data_selectors: - selector_id: blogs selector: \".post-listing .post-item\" selector_attribute: element multiple: true child_selectors: - selector_id: url selector: \".post-header h2 a\" selector_type: css selector_attribute: href multiple: false - selector_id: title selector: \".post-header h2 a\" selector_type: css selector_attribute: text multiple: false - selector_id: content selector: \".post-content\" selector_type: css selector_attribute: html multiple: false traversals: - traversal_type: pagination pagination: selector: \".next-posts-link\" selector_type: css max_pages: 2 next_spider_id: blog_list transformations: - transformation_id: default transformation_fn: transformation_fn callbacks: - callback_id: default data_storage_id: default url: http://localhost/api/callback request_type: POST payload: {} headers: X-TOKEN: abc123456789 data_storages: - data_storage_id: default transformation_id: default connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: blog_list unique_key: url settings: allowed_domains: - blog.scrapinghub.com download_delay: 5 context: author: https://github.com/rrmerugu description: Crawler that scrapes scrapinghub blogs","title":"Crawl, Traverse and Index"},{"location":"tutorials/quickstart/","text":"Quickstart We're going to create a simple single domain spider to get the extract the blogs and save to MongoDB and Elasticsearch. 1. Installation # Create the project directory mkdir tutorial cd tutorial # Create a virtual environment to isolate our package dependencies locally python3 -m venv venv source venv/bin/activate # On Windows use `env\\Scripts\\activate` # Install InvanaBot into the virtual environment pip3 install invana-bot 2. Create Spider # Setup a new spider cat > cti_manifest.yml <<EOF cti_id: scrapinghub_blogs init_spider: start_urls: - \"https://blog.scrapinghub.com\" spider_id: blog_list spiders: - spider_id: blog_list allowed_domains: - \"blog.scrapinghub.com\" traversals: - traversal_id: scrapinghub_pagination selector_type: css selector_value: \".next-posts-link\" max_pages: 1 next_spider_id: blog_list - traversal_id: scrapinghub_detail selector_type: css selector_value: \"h2 a\" max_pages: 1 next_spider_id: blog_detail - spider_id: blog_detail extractors: - extractor_type: CustomContentExtractor extractor_id: blog_detail data_selectors: - selector_id: blog_detail selector: \".blog-section\" selector_attribute: element data_type: DictField child_selectors: - selector_id: title selector: h1 span selector_type: css selector_attribute: text data_type: StringField - selector_id: published_at selector: \".date a\" selector_type: css selector_attribute: text data_type: StringField - selector_id: author selector: \".author a\" selector_type: css selector_attribute: text data_type: StringField data_storages: - data_storage_id: default transformation_id: default connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: scrapinghub_blogs unique_key: url settings: allowed_domains: - \"blog.scrapinghub.com\" download_delay: 0 context: author: https://github.com/rrmerugu description: Crawler that scrapes scrapinghub blogs EOF 3. Run the Spider invana-bot --type=web","title":"Quickstart"},{"location":"tutorials/quickstart/#quickstart","text":"We're going to create a simple single domain spider to get the extract the blogs and save to MongoDB and Elasticsearch.","title":"Quickstart"},{"location":"tutorials/quickstart/#1-installation","text":"# Create the project directory mkdir tutorial cd tutorial # Create a virtual environment to isolate our package dependencies locally python3 -m venv venv source venv/bin/activate # On Windows use `env\\Scripts\\activate` # Install InvanaBot into the virtual environment pip3 install invana-bot","title":"1. Installation"},{"location":"tutorials/quickstart/#2-create-spider","text":"# Setup a new spider cat > cti_manifest.yml <<EOF cti_id: scrapinghub_blogs init_spider: start_urls: - \"https://blog.scrapinghub.com\" spider_id: blog_list spiders: - spider_id: blog_list allowed_domains: - \"blog.scrapinghub.com\" traversals: - traversal_id: scrapinghub_pagination selector_type: css selector_value: \".next-posts-link\" max_pages: 1 next_spider_id: blog_list - traversal_id: scrapinghub_detail selector_type: css selector_value: \"h2 a\" max_pages: 1 next_spider_id: blog_detail - spider_id: blog_detail extractors: - extractor_type: CustomContentExtractor extractor_id: blog_detail data_selectors: - selector_id: blog_detail selector: \".blog-section\" selector_attribute: element data_type: DictField child_selectors: - selector_id: title selector: h1 span selector_type: css selector_attribute: text data_type: StringField - selector_id: published_at selector: \".date a\" selector_type: css selector_attribute: text data_type: StringField - selector_id: author selector: \".author a\" selector_type: css selector_attribute: text data_type: StringField data_storages: - data_storage_id: default transformation_id: default connection_uri: mongodb://127.0.0.1/spiders_data_index collection_name: scrapinghub_blogs unique_key: url settings: allowed_domains: - \"blog.scrapinghub.com\" download_delay: 0 context: author: https://github.com/rrmerugu description: Crawler that scrapes scrapinghub blogs EOF","title":"2. Create Spider"},{"location":"tutorials/quickstart/#3-run-the-spider","text":"invana-bot --type=web","title":"3. Run the Spider"},{"location":"tutorials/single-domain-crawler/","text":"Single Domain Crawler This is the simple form of crawling, this only requires crawling one website of a single structure. Either crawling urls or traversing through pagination. But defined to only one website of single structure. # path should have spider_manifest.yml with the settings and spider_transformations.py # you need to create spider_transformations.py even though you are not performing any transformation. # refer examples/run-single-spider/ folder for reference files that should exist in path. invana-bot --path . --type=single Single Crawler with traversal In this example spider blog_list is paginated for 2 times and extracted the data using the extractors MetaTagExtractor , ParagraphsExtractor , CustomContentExtractor . extra_settings tells few extra settings that will help the crawling run as expected without any block from the site. # spider_manifest.yml spider_id: blog_list start_urls: - https://blog.scrapinghub.com extractors: - extractor_type: MetaTagExtractor extractor_id: meta_tags - extractor_type: ParagraphsExtractor extractor_id: paragraphs - extractor_type: CustomContentExtractor extractor_id: blog_list_parser data_selectors: - selector_id: blogs selector: \".post-listing .post-item\" selector_attribute: element multiple: true child_selectors: - selector_id: url selector: \".post-header h2 a\" selector_type: css selector_attribute: href multiple: false - selector_id: title selector: \".post-header h2 a\" selector_type: css selector_attribute: text multiple: false - selector_id: content selector: \".post-content\" selector_type: css selector_attribute: html multiple: false traversals: - traversal_type: pagination pagination: selector: \".next-posts-link\" selector_type: css max_pages: 2 next_spider_id: blog_list settings: allowed_domains: - blog.scrapinghub.com download_delay: 5 context: author: https://github.com/rrmerugu description: Crawler that scrapes scrapinghub blogs","title":"Single Domain Crawler"},{"location":"tutorials/single-domain-crawler/#single-domain-crawler","text":"This is the simple form of crawling, this only requires crawling one website of a single structure. Either crawling urls or traversing through pagination. But defined to only one website of single structure. # path should have spider_manifest.yml with the settings and spider_transformations.py # you need to create spider_transformations.py even though you are not performing any transformation. # refer examples/run-single-spider/ folder for reference files that should exist in path. invana-bot --path . --type=single","title":"Single Domain Crawler"},{"location":"tutorials/single-domain-crawler/#single-crawler-with-traversal","text":"In this example spider blog_list is paginated for 2 times and extracted the data using the extractors MetaTagExtractor , ParagraphsExtractor , CustomContentExtractor . extra_settings tells few extra settings that will help the crawling run as expected without any block from the site. # spider_manifest.yml spider_id: blog_list start_urls: - https://blog.scrapinghub.com extractors: - extractor_type: MetaTagExtractor extractor_id: meta_tags - extractor_type: ParagraphsExtractor extractor_id: paragraphs - extractor_type: CustomContentExtractor extractor_id: blog_list_parser data_selectors: - selector_id: blogs selector: \".post-listing .post-item\" selector_attribute: element multiple: true child_selectors: - selector_id: url selector: \".post-header h2 a\" selector_type: css selector_attribute: href multiple: false - selector_id: title selector: \".post-header h2 a\" selector_type: css selector_attribute: text multiple: false - selector_id: content selector: \".post-content\" selector_type: css selector_attribute: html multiple: false traversals: - traversal_type: pagination pagination: selector: \".next-posts-link\" selector_type: css max_pages: 2 next_spider_id: blog_list settings: allowed_domains: - blog.scrapinghub.com download_delay: 5 context: author: https://github.com/rrmerugu description: Crawler that scrapes scrapinghub blogs","title":"Single Crawler with traversal"}]}